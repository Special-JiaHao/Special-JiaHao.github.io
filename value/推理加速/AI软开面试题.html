<!-- build time:Mon Sep 09 2024 09:42:49 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="繁華落盡 似水流年" href="http://example.com/rss.xml"><link rel="alternate" type="application/atom+xml" title="繁華落盡 似水流年" href="http://example.com/atom.xml"><link rel="alternate" type="application/json" title="繁華落盡 似水流年" href="http://example.com/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="AI算法"><link rel="canonical" href="http://example.com/value/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/AI%E8%BD%AF%E5%BC%80%E9%9D%A2%E8%AF%95%E9%A2%98.html"><title>AI算法面试题 - AI算法 | Value's Blog = 繁華落盡 似水流年 = 寄予厚望 还请善待</title><meta name="generator" content="Hexo 6.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">AI算法面试题</h1><div class="meta"><span class="item" title="Created: 2024-07-28 14:30:51"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">Posted on</span> <time itemprop="dateCreated datePublished" datetime="2024-07-28T14:30:51+08:00">2024-07-28</time> </span><span class="item" title="Symbols count in article"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">Symbols count in article</span> <span>3.9k</span> <span class="text">words</span> </span><span class="item" title="Reading time"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">Reading time</span> <span>4 mins.</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="Toggle navigation bar"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Value's Blog</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://dogefs.s3.ladydaily.com/~/source/unsplash/photo-1707343843598-39755549ac9a?q=80&w=1932&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDF8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"></li><li class="item" data-background-image="https://raw.githubusercontent.com/Special-JiaHao/images/main/bg.jpg"></li><li class="item" data-background-image="https://browser9.qhimg.com/bdr/__85/t013b5bb8ecdaede745.jpg"></li><li class="item" data-background-image="https://dogefs.s3.ladydaily.com/~/source/unsplash/photo-1679653414131-53146190c320?q=80&w=1974&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"></li><li class="item" data-background-image="https://dogefs.s3.ladydaily.com/~/source/unsplash/photo-1544685051-c47763094b74?q=80&w=2071&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"></li><li class="item" data-background-image="https://images.pexels.com/photos/9024407/pexels-photo-9024407.jpeg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">Home</a></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/AI%E7%AE%97%E6%B3%95/" itemprop="item" rel="index" title="In AI算法"><span itemprop="name">AI算法</span></a><meta itemprop="position" content="1"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="en"><link itemprop="mainEntityOfPage" href="http://example.com/value/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/AI%E8%BD%AF%E5%BC%80%E9%9D%A2%E8%AF%95%E9%A2%98"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Value"><meta itemprop="description" content="寄予厚望 还请善待, 编程日记 & 随笔"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="繁華落盡 似水流年"></span><div class="body md" itemprop="articleBody"><h1 id="ai算法面试题"><a class="anchor" href="#ai算法面试题">#</a> AI 算法面试题</h1><h2 id="简单讲一下-transformer-结构"><a class="anchor" href="#简单讲一下-transformer-结构">#</a> 简单讲一下 Transformer 结构</h2><ul><li><code>Attention</code> 的作用：获取上下文的关系， <code>FFN</code> 的作用：存储知识</li><li>使用的归一化方法是 <code>LayerNorm</code></li><li>计算复杂度和空间复杂度都是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>N</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">N^{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8141079999999999em;vertical-align:0"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.10903em">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>，其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span></span></span></span> 指代的是 <code>seq_len</code></li></ul><ol><li></li></ol><h2 id="bn-与-ln"><a class="anchor" href="#bn-与-ln">#</a> BN 与 LN</h2><blockquote><p>BN 和 LN 均是对数据做正则化，将输入数据归一至正态分布，加速收敛，提高训练的稳定</p></blockquote><h2 id="模型推理过程"><a class="anchor" href="#模型推理过程">#</a> 模型推理过程</h2><p><img data-src="https://raw.githubusercontent.com/Special-JiaHao/images/main/%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E8%BF%87%E7%A8%8B.png" alt=""></p><ul><li></li></ul><h2 id="mha-gqa-mqa"><a class="anchor" href="#mha-gqa-mqa">#</a> MHA、GQA、MQA</h2><ul><li><p><code>MQA(Multi Query Attention)</code> : 让所有的头共享同一份 <code>Key</code> 和 <code>Value</code> 矩阵</p></li><li><p><code>GQA(Grouped Query Attention)</code> : <code>MQA</code> 与 <code>MHA</code> 的择中方案，即不想损失太多性能</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">class</span> <span class="token class-name">GroupedQueryAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">__int__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> num_key_value_heads<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="3"></td><td><pre>        <span class="token builtin">super</span><span class="token punctuation">(</span>GroupedQueryAttention<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size</pre></td></tr><tr><td data-num="5"></td><td><pre>        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads</pre></td></tr><tr><td data-num="6"></td><td><pre>        self<span class="token punctuation">.</span>head_dim <span class="token operator">=</span> hidden_size <span class="token operator">//</span> num_heads</pre></td></tr><tr><td data-num="7"></td><td><pre>        self<span class="token punctuation">.</span>num_key_value_heads <span class="token operator">=</span> num_key_value_heads</pre></td></tr><tr><td data-num="8"></td><td><pre>        self<span class="token punctuation">.</span>num_key_value_groups <span class="token operator">=</span> num_heads <span class="token operator">//</span> num_key_value_heads</pre></td></tr><tr><td data-num="9"></td><td><pre>        self<span class="token punctuation">.</span>q_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim <span class="token operator">*</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre>        self<span class="token punctuation">.</span>k_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim <span class="token operator">*</span> self<span class="token punctuation">.</span>num_key_value_heads<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="11"></td><td><pre>        self<span class="token punctuation">.</span>v_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim <span class="token operator">*</span> self<span class="token punctuation">.</span>num_key_value_heads<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="12"></td><td><pre>        self<span class="token punctuation">.</span>o_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>head_dim <span class="token operator">*</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="13"></td><td><pre>        self<span class="token punctuation">.</span>softmax <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="14"></td><td><pre></pre></td></tr><tr><td data-num="15"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">expand</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="16"></td><td><pre>        batch_size<span class="token punctuation">,</span> seq_len <span class="token operator">=</span> data<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> data<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="17"></td><td><pre>        data <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_heads<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="18"></td><td><pre>                                             self<span class="token punctuation">.</span>num_key_value_groups<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="19"></td><td><pre>        data <span class="token operator">=</span> data<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>num_key_value_groups<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="20"></td><td><pre>                         seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="21"></td><td><pre>        <span class="token keyword">return</span> data</pre></td></tr><tr><td data-num="22"></td><td><pre></pre></td></tr><tr><td data-num="23"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> v<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="24"></td><td><pre>        batch_size<span class="token punctuation">,</span> seq_len <span class="token operator">=</span> q<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> q<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="25"></td><td><pre>        query_states<span class="token punctuation">,</span> key_states<span class="token punctuation">,</span> value_states <span class="token operator">=</span> self<span class="token punctuation">.</span>q_proj<span class="token punctuation">(</span>q<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>k_proj<span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>v_proj<span class="token punctuation">(</span>v<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="26"></td><td><pre>        query_states <span class="token operator">=</span> query_states<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="27"></td><td><pre>        key_states <span class="token operator">=</span> key_states<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="28"></td><td><pre>        value_states <span class="token operator">=</span> value_states<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_key_value_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="29"></td><td><pre>        key_states <span class="token operator">=</span> self<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>key_states<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="30"></td><td><pre>        value_states <span class="token operator">=</span> self<span class="token punctuation">.</span>expand<span class="token punctuation">(</span>value_states<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="31"></td><td><pre>        attn_weights <span class="token operator">=</span> query_states @ key_states<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="32"></td><td><pre>        <span class="token keyword">if</span> mask<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="33"></td><td><pre>            attn_weights <span class="token operator">=</span> attn_weights<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>mask <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1e9</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="34"></td><td><pre>        attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attn_weights<span class="token punctuation">)</span> @ value_states</pre></td></tr><tr><td data-num="35"></td><td><pre>        attn_output <span class="token operator">=</span> attn_output<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="36"></td><td><pre>        attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>o_proj<span class="token punctuation">(</span>attn_output<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="37"></td><td><pre>        <span class="token keyword">return</span> attn_output</pre></td></tr></table></figure></li><li><p><code>MHA(Multi Head Attention)</code> : <code>Q/K/V</code> 三部分具有相同数目的头，且一一对应</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> num_heads<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token boolean">None</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="3"></td><td><pre>        <span class="token builtin">super</span><span class="token punctuation">(</span>MultiHeadAttention<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size</pre></td></tr><tr><td data-num="5"></td><td><pre>        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads</pre></td></tr><tr><td data-num="6"></td><td><pre>        self<span class="token punctuation">.</span>head_dim <span class="token operator">=</span> hidden_size <span class="token operator">//</span> num_heads</pre></td></tr><tr><td data-num="7"></td><td><pre>        self<span class="token punctuation">.</span>q_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim <span class="token operator">*</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>        self<span class="token punctuation">.</span>k_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim <span class="token operator">*</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="9"></td><td><pre>        self<span class="token punctuation">.</span>v_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim <span class="token operator">*</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre>        self<span class="token punctuation">.</span>o_proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>head_dim <span class="token operator">*</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="11"></td><td><pre>        self<span class="token punctuation">.</span>softmax <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="12"></td><td><pre></pre></td></tr><tr><td data-num="13"></td><td><pre>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> v<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="14"></td><td><pre>        batch_size<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> hidden_size <span class="token operator">=</span> q<span class="token punctuation">.</span>shape</pre></td></tr><tr><td data-num="15"></td><td><pre>        query_states<span class="token punctuation">,</span> key_states<span class="token punctuation">,</span> value_states <span class="token operator">=</span> self<span class="token punctuation">.</span>q_proj<span class="token punctuation">(</span>q<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>k_proj<span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>v_proj<span class="token punctuation">(</span>v<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="16"></td><td><pre>        query_states <span class="token operator">=</span> query_states<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="17"></td><td><pre>        key_states <span class="token operator">=</span> key_states<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="18"></td><td><pre>        value_states <span class="token operator">=</span> value_states<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="19"></td><td><pre>        attn_weights <span class="token operator">=</span> query_states @ key_states<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="20"></td><td><pre>        <span class="token keyword">if</span> mask<span class="token punctuation">:</span></pre></td></tr><tr><td data-num="21"></td><td><pre>            mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>trill<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>seq_len<span class="token punctuation">,</span> seq_len<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="22"></td><td><pre>            attn_weights <span class="token operator">=</span> attn_weights<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>mask<span class="token operator">==</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">'-inf'</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="23"></td><td><pre>        attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attn_weights<span class="token punctuation">)</span> @ value_states</pre></td></tr><tr><td data-num="24"></td><td><pre>        attn_output <span class="token operator">=</span> attn_output<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>head_dim<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="25"></td><td><pre>        attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>o_proj<span class="token punctuation">(</span>attn_output<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="26"></td><td><pre>        <span class="token keyword">return</span> attn_output</pre></td></tr></table></figure></li></ul><p><img data-src="https://raw.githubusercontent.com/Special-JiaHao/images/main/mha_mqa_gqa.png" alt=""></p><h2 id="multi-head-latent-attention"><a class="anchor" href="#multi-head-latent-attention">#</a> Multi-Head Latent Attention</h2><p><img data-src="https://raw.githubusercontent.com/Special-JiaHao/images/main/MLA.png" alt=""></p><h2 id="attention的qk之后为什么要除sqrtd_k"><a class="anchor" href="#attention的qk之后为什么要除sqrtd_k">#</a> <code>attention</code> 的 <code>QK</code> 之后为什么要除<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{d_{k}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-.18278000000000005em"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.85722em"><span class="svg-align" style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord" style="padding-left:.833em"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span><span style="top:-2.81722em"><span class="pstrut" style="height:3em"></span><span class="hide-tail" style="min-width:.853em;height:1.08em"><svg width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.18278000000000005em"><span></span></span></span></span></span></span></span></span></h2><p>当<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_{k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.03148em">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span></span></span></span> 维度较大时， <code>Q</code> 与 <code>K</code> 点积后的结果可能很大，会进入到 <code>softmax</code> 的饱和区，而导致梯度变得非常小，会使得训练变得困难。缩放有助于维持点积后结果的稳定性，确保梯度维持在一个合理的范围内.</p><h2 id="位置编码"><a class="anchor" href="#位置编码">#</a> 位置编码</h2><ul><li><code>Transformer</code> 中的位置编码</li></ul><h2 id="transformer-中的梯度裁剪"><a class="anchor" href="#transformer-中的梯度裁剪">#</a> Transformer 中的梯度裁剪</h2><h2 id="transformer-中的学习率预热策略"><a class="anchor" href="#transformer-中的学习率预热策略">#</a> Transformer 中的学习率预热策略</h2><h2 id="prenorm-与-postnorm的区别"><a class="anchor" href="#prenorm-与-postnorm的区别">#</a> <code>preNorm</code> 与 <code>postNorm</code> 的区别</h2><p><img data-src="https://raw.githubusercontent.com/Special-JiaHao/images/main/postNorm%E4%B8%8EPreNorm.png" alt=""></p><ul><li><code>Post-LN</code> 是在残差之后做归一化，对参数正则化效果好，进而具有更强的鲁棒性</li><li><code>Pre-LN</code> 在残差之前做归一化，有一部分数据直接加到了后面，不需要对这部分参数做正则化，正好可以防止模型的梯度消失或梯度爆炸</li></ul><blockquote><p>如果层数少 <code>Post-LN</code> 的效果更好一些，如果把层数拉大，为了保证模型的训练， <code>Pre-LN</code> 要好一些</p></blockquote><h2 id="为什么下载的-llm-大多数都是-decoder-only-结构"><a class="anchor" href="#为什么下载的-llm-大多数都是-decoder-only-结构">#</a> 为什么下载的 LLM 大多数都是 Decoder Only 结构</h2><div class="tags"><a href="/tags/AI%E7%AE%97%E6%B3%95/" rel="tag"><i class="ic i-tag"></i> AI算法</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">Edited on</span> <time title="Modified: 2024-09-04 17:01:31" itemprop="dateModified" datetime="2024-09-04T17:01:31+08:00">2024-09-04</time> </span><span id="value/推理加速/AI软开面试题.html" class="item leancloud_visitors" data-flag-title="AI算法面试题" title="Views"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">Views</span> <span class="leancloud-visitors-count"></span> <span class="text">times</span></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> Donate</button><p>Give me a cup of [coffee]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.jpg" alt="Value WeChat Pay"><p>WeChat Pay</p></div></div></div><div id="copyright"><ul><li class="author"><strong>Post author: </strong>Value <i class="ic i-at"><em>@</em></i>繁華落盡 似水流年</li><li class="link"><strong>Post link: </strong><a href="http://example.com/value/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/AI%E8%BD%AF%E5%BC%80%E9%9D%A2%E8%AF%95%E9%A2%98" title="AI算法面试题">http://example.com/value/推理加速/AI软开面试题</a></li><li class="license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> unless stating additionally.</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/value/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/FlashAttention" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;Special-JiaHao&#x2F;images&#x2F;main&#x2F;FlashAttention.jpg" title="Flash Attention"><span class="type">Previous Post</span> <span class="category"><i class="ic i-flag"></i> 推理加速</span><h3>Flash Attention</h3></a></div><div class="item right"><a href="/value/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/Paged%20Attention" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;dogefs.s3.ladydaily.com&#x2F;~&#x2F;source&#x2F;unsplash&#x2F;photo-1595324589142-39342e154ca7?q&#x3D;80&amp;w&#x3D;1887&amp;auto&#x3D;format&amp;fit&#x3D;crop&amp;ixlib&#x3D;rb-4.0.3&amp;ixid&#x3D;M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D" title="Paged Attention"><span class="type">Next Post</span> <span class="category"><i class="ic i-flag"></i> 推理加速</span><h3>Paged Attention</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="Contents"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#ai%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%A2%98"><span class="toc-number">1.</span> <span class="toc-text">AI 算法面试题</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E5%8D%95%E8%AE%B2%E4%B8%80%E4%B8%8B-transformer-%E7%BB%93%E6%9E%84"><span class="toc-number">1.1.</span> <span class="toc-text">简单讲一下 Transformer 结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#bn-%E4%B8%8E-ln"><span class="toc-number">1.2.</span> <span class="toc-text">BN 与 LN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E8%BF%87%E7%A8%8B"><span class="toc-number">1.3.</span> <span class="toc-text">模型推理过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#mha-gqa-mqa"><span class="toc-number">1.4.</span> <span class="toc-text">MHA、GQA、MQA</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#multi-head-latent-attention"><span class="toc-number">1.5.</span> <span class="toc-text">Multi-Head Latent Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#attention%E7%9A%84qk%E4%B9%8B%E5%90%8E%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E9%99%A4sqrtd_k"><span class="toc-number">1.6.</span> <span class="toc-text">attention 的 QK 之后为什么要除dk\sqrt{d_{k}}dk​​</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">1.7.</span> <span class="toc-text">位置编码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#transformer-%E4%B8%AD%E7%9A%84%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AA"><span class="toc-number">1.8.</span> <span class="toc-text">Transformer 中的梯度裁剪</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#transformer-%E4%B8%AD%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%8E%87%E9%A2%84%E7%83%AD%E7%AD%96%E7%95%A5"><span class="toc-number">1.9.</span> <span class="toc-text">Transformer 中的学习率预热策略</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#prenorm-%E4%B8%8E-postnorm%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">1.10.</span> <span class="toc-text">preNorm 与 postNorm 的区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8B%E8%BD%BD%E7%9A%84-llm-%E5%A4%A7%E5%A4%9A%E6%95%B0%E9%83%BD%E6%98%AF-decoder-only-%E7%BB%93%E6%9E%84"><span class="toc-number">1.11.</span> <span class="toc-text">为什么下载的 LLM 大多数都是 Decoder Only 结构</span></a></li></ol></li></ol></div><div class="related panel pjax" data-title="Related"><ul><li class="active"><a href="/value/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/AI%E8%BD%AF%E5%BC%80%E9%9D%A2%E8%AF%95%E9%A2%98" rel="bookmark" title="AI算法面试题">AI算法面试题</a></li></ul></div><div class="overview panel" data-title="Overview"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Value" data-src="/images/avatar.jpg"><p class="name" itemprop="name">Value</p><div class="description" itemprop="description">编程日记 & 随笔</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">64</span> <span class="name">posts</span></a></div><div class="item categories"><a href="/categories/"><span class="count">53</span> <span class="name">categories</span></a></div><div class="item tags"><a href="/tags/"><span class="count">40</span> <span class="name">tags</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL1NwZWNpYWwtSmlhSGFv" title="https:&#x2F;&#x2F;github.com&#x2F;Special-JiaHao"><i class="ic i-github"></i></span> <span class="exturl item music" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvdXNlci9ob21lP2lkPTIwNDE4OTkzODk=" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;2041899389"><i class="ic i-cloud-music"></i></span> <span class="exturl item bilibili" data-url="aHR0cHM6Ly9zcGFjZS5iaWxpYmlsaS5jb20vNDMzNDE3NDUwP3NwbV9pZF9mcm9tPTMzMy4zMzcuMC4w" title="https:&#x2F;&#x2F;space.bilibili.com&#x2F;433417450?spm_id_from&#x3D;333.337.0.0"><i class="ic i-bilibili"></i></span> <span class="exturl item email" data-url="bWFpbHRvOmpoMTU3NzA5QDE2My5jb20=" title="mailto:jh157709@163.com"><i class="ic i-envelope"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>Home</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>About</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>Posts</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>Archives</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>Categories</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>Tags</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>friends</a></li><li class="item"><a href="/links/" rel="section"><i class="ic i-magic"></i>links</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/value/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/FlashAttention" rel="prev" title="Previous Post"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/value/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/Paged%20Attention" rel="next" title="Next Post"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>Random Posts</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/%E6%A0%A1%E6%8B%9B/" title="In 校招">校招</a> <i class="ic i-angle-right"></i> <a href="/categories/%E6%A0%A1%E6%8B%9B/%E8%9A%82%E8%9A%81%E9%9B%86%E5%9B%A2/" title="In 蚂蚁集团">蚂蚁集团</a></div><span><a href="/value/%E6%A0%A1%E6%8B%9B%E4%B8%8E%E9%9D%A2%E8%AF%95/%E8%9A%82%E8%9A%81%E9%9B%86%E5%9B%A2%E6%A0%A1%E6%8B%9B" title="蚂蚁集团校招">蚂蚁集团校招</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Code/" title="In Code">Code</a> <i class="ic i-angle-right"></i> <a href="/categories/Code/%E7%89%9B%E5%AE%A2/" title="In 牛客">牛客</a></div><span><a href="/value/Code/%E7%89%9B%E5%AE%A2%E5%B0%8F%E7%99%BD%E6%9C%88%E8%B5%9B93%20" title="牛客小白月赛93">牛客小白月赛93</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" title="In 操作系统">操作系统</a></div><span><a href="/value/Linux%E5%9F%BA%E7%A1%80%E4%B8%8E%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4" title="Linux基础与常用命令">Linux基础与常用命令</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/daily/" title="In daily">daily</a> <i class="ic i-angle-right"></i> <a href="/categories/daily/June/" title="In June">June</a></div><span><a href="/value/daily/June" title="April 2024">April 2024</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/CPP/" title="In C++">C++</a> <i class="ic i-angle-right"></i> <a href="/categories/CPP/Project/" title="In Project">Project</a></div><span><a href="/value/CPP/%E9%87%91%E5%B1%B1%E4%BC%9A%E8%AE%AE%E5%AE%A4%E9%A2%84%E7%BA%A6%E7%B3%BB%E7%BB%9F" title="金山会议室预约系统">金山会议室预约系统</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/CPP/" title="In C++">C++</a></div><span><a href="/value/CPP/IO%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8" title="IO多路复用">IO多路复用</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Go/" title="In Go">Go</a></div><span><a href="/value/Go/go%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95" title="Go基础语法">Go基础语法</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/Deep-Learning/" title="In Deep Learning">Deep Learning</a></div><span><a href="/value/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0" title="激活函数">激活函数</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/ns3/" title="In ns3">ns3</a></div><span><a href="/value/ns3%20install" title="ns3 install">ns3 install</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" title="In 设计模式">设计模式</a></div><span><a href="/value/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F" title="设计模式">设计模式</a></span></li></ul></div><div><h2>Recent Comments</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Value @ Value's Blog</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="Symbols count total">208k words</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="Reading time total">3:09</span></div><div class="powered-by">Powered by <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"value/推理加速/AI软开面试题.html",favicon:{show:"（●´3｀●）Goooood",hide:"(´Д｀)Booooom"},search:{placeholder:"Search for Posts",empty:"We didn't find any results for the search: ${query}",stats:"${hits} results found in ${time} ms"},valine:!0,copy_tex:!0,katex:!0,fancybox:!0,copyright:'Copied to clipboard successfully! <br> All articles in this blog are licensed under <i class="ic i-creative-commons"></i>BY-NC-SA.',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html><!-- rebuild by hrmmi -->