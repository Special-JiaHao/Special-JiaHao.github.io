{
    "version": "https://jsonfeed.org/version/1",
    "title": "繁華落盡 似水流年 • All posts by \"ai算法\" category",
    "description": "编程日记 & 随笔",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/value/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/AI%E8%BD%AF%E5%BC%80%E9%9D%A2%E8%AF%95%E9%A2%98",
            "url": "http://example.com/value/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/AI%E8%BD%AF%E5%BC%80%E9%9D%A2%E8%AF%95%E9%A2%98",
            "title": "AI算法面试题",
            "date_published": "2024-07-28T06:30:51.559Z",
            "content_html": "<h1 id=\"ai算法面试题\"><a class=\"anchor\" href=\"#ai算法面试题\">#</a> AI 算法面试题</h1>\n<h2 id=\"简单讲一下-transformer-结构\"><a class=\"anchor\" href=\"#简单讲一下-transformer-结构\">#</a> 简单讲一下 Transformer 结构</h2>\n<ul>\n<li><code>Attention</code>  的作用：获取上下文的关系， <code>FFN</code>  的作用：存储知识</li>\n<li>使用的归一化方法是 <code>LayerNorm</code></li>\n<li>计算复杂度和空间复杂度都是<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>N</mi><mn>2</mn></msup></mrow><annotation encoding=\"application/x-tex\">N^{2}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8141079999999999em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span></span>，其中<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span> 指代的是 <code>seq_len</code></li>\n</ul>\n<ol>\n<li></li>\n</ol>\n<h2 id=\"bn-与-ln\"><a class=\"anchor\" href=\"#bn-与-ln\">#</a> BN 与 LN</h2>\n<blockquote>\n<p>BN 和 LN 均是对数据做正则化，将输入数据归一至正态分布，加速收敛，提高训练的稳定</p>\n</blockquote>\n<h2 id=\"模型推理过程\"><a class=\"anchor\" href=\"#模型推理过程\">#</a> 模型推理过程</h2>\n<p><img data-src=\"https://raw.githubusercontent.com/Special-JiaHao/images/main/%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E8%BF%87%E7%A8%8B.png\" alt=\"\" /></p>\n<ul>\n<li></li>\n</ul>\n<h2 id=\"mha-gqa-mqa\"><a class=\"anchor\" href=\"#mha-gqa-mqa\">#</a> MHA、GQA、MQA</h2>\n<ul>\n<li><code>MQA(Multi Query Attention)</code> : 让所有的头共享同一份 <code>Key</code>  和 <code>Value</code>  矩阵</li>\n<li><code>GQA(Grouped Query Attention)</code> :  <code>MQA</code>  与 <code>MHA</code>  的择中方案，即不想损失太多性能</li>\n<li><code>MHA(Multi Head Attention)</code> : <code>Q/K/V</code>  三部分具有相同数目的头，且一一对应</li>\n</ul>\n<p><img data-src=\"https://raw.githubusercontent.com/Special-JiaHao/images/main/mha_mqa_gqa.png\" alt=\"\" /></p>\n<h2 id=\"attention的qk之后为什么要除sqrtd_k\"><a class=\"anchor\" href=\"#attention的qk之后为什么要除sqrtd_k\">#</a>  <code>attention</code>  的 <code>QK</code>  之后为什么要除<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow><annotation encoding=\"application/x-tex\">\\sqrt{d_{k}}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.04em;vertical-align:-0.18278000000000005em;\"></span><span class=\"mord sqrt\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.85722em;\"><span class=\"svg-align\" style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\" style=\"padding-left:0.833em;\"><span class=\"mord\"><span class=\"mord mathnormal\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-2.81722em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"hide-tail\" style=\"min-width:0.853em;height:1.08em;\"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702\nc-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14\nc0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54\nc44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10\ns173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429\nc69,-144,104.5,-217.7,106.5,-221\nl0 -0\nc5.3,-9.3,12,-14,20,-14\nH400000v40H845.2724\ns-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7\nc-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z\nM834 80h400000v40h-400000z'/></svg></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.18278000000000005em;\"><span></span></span></span></span></span></span></span></span></h2>\n<p>当<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding=\"application/x-tex\">d_{k}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.84444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">d</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 维度较大时， <code>Q</code>  与 <code>K</code>  点积后的结果可能很大，会进入到 <code>softmax</code>  的饱和区，而导致梯度变得非常小，会使得训练变得困难。缩放有助于维持点积后结果的稳定性，确保梯度维持在一个合理的范围内.</p>\n<h2 id=\"位置编码\"><a class=\"anchor\" href=\"#位置编码\">#</a> 位置编码</h2>\n<ul>\n<li><code>Transformer</code>  中的位置编码</li>\n</ul>\n<h2 id=\"transformer-中的梯度裁剪\"><a class=\"anchor\" href=\"#transformer-中的梯度裁剪\">#</a> Transformer 中的梯度裁剪</h2>\n<h2 id=\"transformer-中的学习率预热策略\"><a class=\"anchor\" href=\"#transformer-中的学习率预热策略\">#</a> Transformer 中的学习率预热策略</h2>\n<h2 id=\"prenorm-与-postnorm的区别\"><a class=\"anchor\" href=\"#prenorm-与-postnorm的区别\">#</a>  <code>preNorm</code>  与  <code>postNorm</code>  的区别</h2>\n<p><img data-src=\"https://raw.githubusercontent.com/Special-JiaHao/images/main/postNorm%E4%B8%8EPreNorm.png\" alt=\"\" /></p>\n<ul>\n<li><code>Post-LN</code>  是在残差之后做归一化，对参数正则化效果好，进而具有更强的鲁棒性</li>\n<li><code>Pre-LN</code>  在残差之前做归一化，有一部分数据直接加到了后面，不需要对这部分参数做正则化，正好可以防止模型的梯度消失或梯度爆炸</li>\n</ul>\n<blockquote>\n<p>如果层数少 <code>Post-LN</code>  的效果更好一些，如果把层数拉大，为了保证模型的训练， <code>Pre-LN</code>  要好一些</p>\n</blockquote>\n<h2 id=\"为什么下载的-llm-大多数都是-decoder-only-结构\"><a class=\"anchor\" href=\"#为什么下载的-llm-大多数都是-decoder-only-结构\">#</a> 为什么下载的 LLM 大多数都是 Decoder Only 结构</h2>\n",
            "tags": [
                "AI算法"
            ]
        }
    ]
}