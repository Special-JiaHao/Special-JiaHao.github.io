<?xml version="1.0"?>
<rss version="2.0">
    <channel>
        <title>繁華落盡 似水流年 • Posts by &#34;ai算法&#34; category</title>
        <link>http://example.com</link>
        <description>编程日记 &amp; 随笔</description>
        <language>en</language>
        <pubDate>Sun, 28 Jul 2024 14:30:51 +0800</pubDate>
        <lastBuildDate>Sun, 28 Jul 2024 14:30:51 +0800</lastBuildDate>
        <category>mathematics</category>
        <category>语法</category>
        <category>C++</category>
        <category>VMware</category>
        <category>install</category>
        <category>数据库</category>
        <category>STL</category>
        <category>CNN</category>
        <category>Project</category>
        <category>面试题</category>
        <category>日常</category>
        <category>校招</category>
        <category>算法</category>
        <category>Leetcode双周赛</category>
        <category>Algorithm</category>
        <category>Leetcode周赛</category>
        <category>设计模式</category>
        <category>多线程</category>
        <category>池化技术</category>
        <category>MySQL</category>
        <category>操作系统</category>
        <category>计算机网络</category>
        <category>CMake</category>
        <category>线程池</category>
        <category>Go</category>
        <category>Redis</category>
        <category>Linux</category>
        <category>json</category>
        <category>Python</category>
        <category>牛客</category>
        <category>Git</category>
        <category>normalization</category>
        <category>推理加速</category>
        <category>LLM</category>
        <category>AI算法</category>
        <category>Pytorch</category>
        <category>激活函数</category>
        <category>部署</category>
        <category>DolphinScheduler</category>
        <item>
            <guid isPermalink="true">http://example.com/value/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/AI%E8%BD%AF%E5%BC%80%E9%9D%A2%E8%AF%95%E9%A2%98</guid>
            <title>AI算法面试题</title>
            <link>http://example.com/value/%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/AI%E8%BD%AF%E5%BC%80%E9%9D%A2%E8%AF%95%E9%A2%98</link>
            <category>AI算法</category>
            <pubDate>Sun, 28 Jul 2024 14:30:51 +0800</pubDate>
            <description><![CDATA[ &lt;h1 id=&#34;ai算法面试题&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#ai算法面试题&#34;&gt;#&lt;/a&gt; AI 算法面试题&lt;/h1&gt;
&lt;h2 id=&#34;简单讲一下-transformer-结构&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#简单讲一下-transformer-结构&#34;&gt;#&lt;/a&gt; 简单讲一下 Transformer 结构&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Attention&lt;/code&gt;  的作用：获取上下文的关系， &lt;code&gt;FFN&lt;/code&gt;  的作用：存储知识&lt;/li&gt;
&lt;li&gt;使用的归一化方法是 &lt;code&gt;LayerNorm&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;计算复杂度和空间复杂度都是&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;N^{2}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.8141079999999999em;vertical-align:0em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.10903em;&#34;&gt;N&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.8141079999999999em;&#34;&gt;&lt;span style=&#34;top:-3.063em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;，其中&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;N&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.68333em;vertical-align:0em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord mathnormal&#34; style=&#34;margin-right:0.10903em;&#34;&gt;N&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 指代的是 &lt;code&gt;seq_len&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;bn-与-ln&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#bn-与-ln&#34;&gt;#&lt;/a&gt; BN 与 LN&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;BN 和 LN 均是对数据做正则化，将输入数据归一至正态分布，加速收敛，提高训练的稳定&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;模型推理过程&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#模型推理过程&#34;&gt;#&lt;/a&gt; 模型推理过程&lt;/h2&gt;
&lt;p&gt;&lt;img data-src=&#34;https://raw.githubusercontent.com/Special-JiaHao/images/main/%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E8%BF%87%E7%A8%8B.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;mha-gqa-mqa&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#mha-gqa-mqa&#34;&gt;#&lt;/a&gt; MHA、GQA、MQA&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;MQA(Multi Query Attention)&lt;/code&gt; : 让所有的头共享同一份 &lt;code&gt;Key&lt;/code&gt;  和 &lt;code&gt;Value&lt;/code&gt;  矩阵&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GQA(Grouped Query Attention)&lt;/code&gt; :  &lt;code&gt;MQA&lt;/code&gt;  与 &lt;code&gt;MHA&lt;/code&gt;  的择中方案，即不想损失太多性能&lt;/li&gt;
&lt;li&gt;&lt;code&gt;MHA(Multi Head Attention)&lt;/code&gt; : &lt;code&gt;Q/K/V&lt;/code&gt;  三部分具有相同数目的头，且一一对应&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img data-src=&#34;https://raw.githubusercontent.com/Special-JiaHao/images/main/mha_mqa_gqa.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;attention的qk之后为什么要除sqrtd_k&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#attention的qk之后为什么要除sqrtd_k&#34;&gt;#&lt;/a&gt;  &lt;code&gt;attention&lt;/code&gt;  的 &lt;code&gt;QK&lt;/code&gt;  之后为什么要除&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msqrt&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;\sqrt{d_{k}}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:1.04em;vertical-align:-0.18278000000000005em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord sqrt&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.85722em;&#34;&gt;&lt;span class=&#34;svg-align&#34; style=&#34;top:-3em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34; style=&#34;padding-left:0.833em;&#34;&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.33610799999999996em;&#34;&gt;&lt;span style=&#34;top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.03148em;&#34;&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;top:-2.81722em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:3em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;hide-tail&#34; style=&#34;min-width:0.853em;height:1.08em;&#34;&gt;&lt;svg width=&#39;400em&#39; height=&#39;1.08em&#39; viewBox=&#39;0 0 400000 1080&#39; preserveAspectRatio=&#39;xMinYMin slice&#39;&gt;&lt;path d=&#39;M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z&#39;/&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.18278000000000005em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;当&lt;span class=&#34;katex&#34;&gt;&lt;span class=&#34;katex-mathml&#34;&gt;&lt;math xmlns=&#34;http://www.w3.org/1998/Math/MathML&#34;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding=&#34;application/x-tex&#34;&gt;d_{k}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&#34;katex-html&#34; aria-hidden=&#34;true&#34;&gt;&lt;span class=&#34;base&#34;&gt;&lt;span class=&#34;strut&#34; style=&#34;height:0.84444em;vertical-align:-0.15em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;mord&#34;&gt;&lt;span class=&#34;mord mathnormal&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;msupsub&#34;&gt;&lt;span class=&#34;vlist-t vlist-t2&#34;&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.33610799999999996em;&#34;&gt;&lt;span style=&#34;top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;&#34;&gt;&lt;span class=&#34;pstrut&#34; style=&#34;height:2.7em;&#34;&gt;&lt;/span&gt;&lt;span class=&#34;sizing reset-size6 size3 mtight&#34;&gt;&lt;span class=&#34;mord mtight&#34;&gt;&lt;span class=&#34;mord mathnormal mtight&#34; style=&#34;margin-right:0.03148em;&#34;&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-s&#34;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;vlist-r&#34;&gt;&lt;span class=&#34;vlist&#34; style=&#34;height:0.15em;&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; 维度较大时， &lt;code&gt;Q&lt;/code&gt;  与 &lt;code&gt;K&lt;/code&gt;  点积后的结果可能很大，会进入到 &lt;code&gt;softmax&lt;/code&gt;  的饱和区，而导致梯度变得非常小，会使得训练变得困难。缩放有助于维持点积后结果的稳定性，确保梯度维持在一个合理的范围内.&lt;/p&gt;
&lt;h2 id=&#34;位置编码&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#位置编码&#34;&gt;#&lt;/a&gt; 位置编码&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Transformer&lt;/code&gt;  中的位置编码&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;transformer-中的梯度裁剪&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#transformer-中的梯度裁剪&#34;&gt;#&lt;/a&gt; Transformer 中的梯度裁剪&lt;/h2&gt;
&lt;h2 id=&#34;transformer-中的学习率预热策略&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#transformer-中的学习率预热策略&#34;&gt;#&lt;/a&gt; Transformer 中的学习率预热策略&lt;/h2&gt;
&lt;h2 id=&#34;prenorm-与-postnorm的区别&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#prenorm-与-postnorm的区别&#34;&gt;#&lt;/a&gt;  &lt;code&gt;preNorm&lt;/code&gt;  与  &lt;code&gt;postNorm&lt;/code&gt;  的区别&lt;/h2&gt;
&lt;p&gt;&lt;img data-src=&#34;https://raw.githubusercontent.com/Special-JiaHao/images/main/postNorm%E4%B8%8EPreNorm.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Post-LN&lt;/code&gt;  是在残差之后做归一化，对参数正则化效果好，进而具有更强的鲁棒性&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Pre-LN&lt;/code&gt;  在残差之前做归一化，有一部分数据直接加到了后面，不需要对这部分参数做正则化，正好可以防止模型的梯度消失或梯度爆炸&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;如果层数少 &lt;code&gt;Post-LN&lt;/code&gt;  的效果更好一些，如果把层数拉大，为了保证模型的训练， &lt;code&gt;Pre-LN&lt;/code&gt;  要好一些&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;为什么下载的-llm-大多数都是-decoder-only-结构&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#为什么下载的-llm-大多数都是-decoder-only-结构&#34;&gt;#&lt;/a&gt; 为什么下载的 LLM 大多数都是 Decoder Only 结构&lt;/h2&gt;
 ]]></description>
        </item>
    </channel>
</rss>
