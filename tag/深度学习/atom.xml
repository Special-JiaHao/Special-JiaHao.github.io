<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>http://example.com</id>
    <title>繁華落盡 似水流年 • Posts by &#34;深度学习&#34; tag</title>
    <link href="http://example.com" />
    <updated>2024-07-22T07:24:22.324Z</updated>
    <category term="mathematics" />
    <category term="语法" />
    <category term="C++" />
    <category term="VMware" />
    <category term="install" />
    <category term="数据库" />
    <category term="STL" />
    <category term="CNN" />
    <category term="Project" />
    <category term="面试题" />
    <category term="日常" />
    <category term="校招" />
    <category term="算法" />
    <category term="Leetcode双周赛" />
    <category term="Algorithm" />
    <category term="Leetcode周赛" />
    <category term="设计模式" />
    <category term="多线程" />
    <category term="池化技术" />
    <category term="MySQL" />
    <category term="操作系统" />
    <category term="计算机网络" />
    <category term="CMake" />
    <category term="线程池" />
    <category term="Go" />
    <category term="Redis" />
    <category term="Linux" />
    <category term="json" />
    <category term="Python" />
    <category term="牛客" />
    <category term="Git" />
    <category term="大模型" />
    <category term="深度学习" />
    <entry>
        <id>http://example.com/value/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Normalization</id>
        <title>Normalization</title>
        <link rel="alternate" href="http://example.com/value/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Normalization"/>
        <content type="html">&lt;h1 id=&#34;normalization&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#normalization&#34;&gt;#&lt;/a&gt; Normalization&lt;/h1&gt;
&lt;h2 id=&#34;ics&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#ics&#34;&gt;#&lt;/a&gt; ICS&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;ICS&lt;/strong&gt;: 协变量，Internal Covariate Shift. 在训练多层神经网络，每一层的神经网络在训练过程当中，它的参数是会发生变动的，前一层网络参数的变动会影响下一层的输入数据，如果数据分布发生重大变化，就会导致训练过程中收敛慢、不稳定的问题.  （要想训练效果好，需要采取一些小心翼翼的策略，比如减小学习率，精心初始化参数，有时还需要 dropout 技巧）&lt;/p&gt;
&lt;h2 id=&#34;batch-normalization1&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#batch-normalization1&#34;&gt;#&lt;/a&gt; Batch Normalization[^1]&lt;/h2&gt;
&lt;p&gt;一种加速深度神经网络训练速度的方法，同事可以减少 ICS 问题。可以使得在训练时大胆的使用大学习率和初始化参数，在一定程度上可以&lt;/p&gt;
&lt;h2 id=&#34;参考文章&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#参考文章&#34;&gt;#&lt;/a&gt; 参考文章&lt;/h2&gt;
&lt;p&gt;[^ 1] [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift]([&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE1MDIuMDMxNjc=&#34;&gt;1502.03167] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (arxiv.org)&lt;/span&gt;)&lt;/p&gt;
</content>
        <category term="深度学习" />
        <updated>2024-07-22T07:24:22.324Z</updated>
    </entry>
</feed>
