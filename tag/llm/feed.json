{
    "version": "https://jsonfeed.org/version/1",
    "title": "繁華落盡 似水流年 • All posts by \"llm\" tag",
    "description": "编程日记 & 随笔",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/value/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/GPT",
            "url": "http://example.com/value/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/GPT",
            "title": "GPT",
            "date_published": "2024-09-03T00:55:58.397Z",
            "content_html": "<h1 id=\"gpt\"><a class=\"anchor\" href=\"#gpt\">#</a> GPT</h1>\n",
            "tags": [
                "LLM"
            ]
        },
        {
            "id": "http://example.com/value/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Llama",
            "url": "http://example.com/value/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Llama",
            "title": "Llama",
            "date_published": "2024-07-22T02:34:52.107Z",
            "content_html": "<h1 id=\"llama\"><a class=\"anchor\" href=\"#llama\">#</a> Llama</h1>\n<h2 id=\"创新点\"><a class=\"anchor\" href=\"#创新点\">#</a> 创新点</h2>\n<ul>\n<li><code>Pre-Normalization</code> ：前置层归一化，可以让训练更加稳定，防止梯度消失的问题.</li>\n<li><code>RMSNorm</code> ：均方根归一化，相比于 <code>Layer-Normalization</code></li>\n<li><code>RoPE</code> ：旋转位置编码，与传统的 <code>Transform</code>  模型的固定位置编码不同，其是一种动态的位置编码.  <code>RoPE</code>  不直接嵌入词向量中，而是对 <code>QKV</code>  做了选择操作，从而引入对应词的位置信息.</li>\n<li><code>SwiGLU</code> ：激活函数。结合了 <code>GLU</code>  和 <code>Switch</code>  的优势.</li>\n</ul>\n",
            "tags": [
                "LLM"
            ]
        }
    ]
}