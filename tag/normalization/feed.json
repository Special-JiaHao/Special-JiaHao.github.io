{
    "version": "https://jsonfeed.org/version/1",
    "title": "繁華落盡 似水流年 • All posts by \"normalization\" tag",
    "description": "编程日记 & 随笔",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/value/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Normalization",
            "url": "http://example.com/value/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Normalization",
            "title": "Normalization",
            "date_published": "2024-07-22T07:24:22.324Z",
            "content_html": "<h1 id=\"normalization\"><a class=\"anchor\" href=\"#normalization\">#</a> Normalization</h1>\n<h2 id=\"ics\"><a class=\"anchor\" href=\"#ics\">#</a> ICS</h2>\n<p><strong>ICS</strong>: 内部协变量转移， <code>Internal Covariate Shift</code> . 在训练多层神经网络，每一层的神经网络在训练过程当中，它的参数是会发生变动的，前一层网络参数的变动会影响下一层的输入数据，如果数据分布发生重大变化，就会导致训练过程中收敛慢、不稳定的问题。固为了达到好的训练效果，需要采取一些小心翼翼的策略，比如减小学习率，精心初始化参数，有时还需要 <code>Dropout</code>  技巧.</p>\n<h2 id=\"batch-normalization\"><a class=\"anchor\" href=\"#batch-normalization\">#</a> Batch Normalization<sup class=\"footnote-ref\"><a href=\"#fn1\" id=\"fnref1\">[1]</a></sup></h2>\n<p>批归一化 <code>BatchNorm</code> , 一种加速深度神经网络训练速度的方法。同时可以减少 <code>ICS</code>  问题。可以使得在训练时大胆的使用跟高的学习率和初始化参数，在一定程度上可以具有正则化效果，防止过拟合的功效.（一定程度与 <code>Dropout</code>  等效） <img data-src=\"https://raw.githubusercontent.com/Special-JiaHao/images/main/bn.png\" alt=\"\" /></p>\n<h2 id=\"layer-normalization\"><a class=\"anchor\" href=\"#layer-normalization\">#</a> Layer Normalization</h2>\n<h2 id=\"参考文章\"><a class=\"anchor\" href=\"#参考文章\">#</a> 参考文章</h2>\n<hr class=\"footnotes-sep\" />\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn1\" class=\"footnote-item\"><p>[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift]([<span class=\"exturl\" data-url=\"aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE1MDIuMDMxNjc=\">1502.03167] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (arxiv.org)</span>) <a href=\"#fnref1\" class=\"footnote-backref\">↩︎</a></p>\n</li>\n</ol>\n</section>\n",
            "tags": [
                "normalization"
            ]
        }
    ]
}