<?xml version="1.0"?>
<rss version="2.0">
    <channel>
        <title>繁華落盡 似水流年 • Posts by &#34;normalization&#34; tag</title>
        <link>http://example.com</link>
        <description>编程日记 &amp; 随笔</description>
        <language>en</language>
        <pubDate>Mon, 22 Jul 2024 15:24:22 +0800</pubDate>
        <lastBuildDate>Mon, 22 Jul 2024 15:24:22 +0800</lastBuildDate>
        <category>mathematics</category>
        <category>语法</category>
        <category>C++</category>
        <category>VMware</category>
        <category>install</category>
        <category>数据库</category>
        <category>STL</category>
        <category>CNN</category>
        <category>Project</category>
        <category>面试题</category>
        <category>日常</category>
        <category>校招</category>
        <category>算法</category>
        <category>Leetcode双周赛</category>
        <category>Algorithm</category>
        <category>Leetcode周赛</category>
        <category>设计模式</category>
        <category>多线程</category>
        <category>池化技术</category>
        <category>MySQL</category>
        <category>操作系统</category>
        <category>计算机网络</category>
        <category>CMake</category>
        <category>线程池</category>
        <category>Go</category>
        <category>Redis</category>
        <category>Linux</category>
        <category>json</category>
        <category>Python</category>
        <category>牛客</category>
        <category>Git</category>
        <category>normalization</category>
        <category>推理加速</category>
        <category>LLM</category>
        <category>AI算法</category>
        <category>Pytorch</category>
        <category>激活函数</category>
        <category>部署</category>
        <category>DolphinScheduler</category>
        <item>
            <guid isPermalink="true">http://example.com/value/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Normalization</guid>
            <title>Normalization</title>
            <link>http://example.com/value/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Normalization</link>
            <category>normalization</category>
            <pubDate>Mon, 22 Jul 2024 15:24:22 +0800</pubDate>
            <description><![CDATA[ &lt;h1 id=&#34;normalization&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#normalization&#34;&gt;#&lt;/a&gt; Normalization&lt;/h1&gt;
&lt;h2 id=&#34;ics&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#ics&#34;&gt;#&lt;/a&gt; ICS&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;ICS&lt;/strong&gt;: 内部协变量转移， &lt;code&gt;Internal Covariate Shift&lt;/code&gt; . 在训练多层神经网络，每一层的神经网络在训练过程当中，它的参数是会发生变动的，前一层网络参数的变动会影响下一层的输入数据，如果数据分布发生重大变化，就会导致训练过程中收敛慢、不稳定的问题。固为了达到好的训练效果，需要采取一些小心翼翼的策略，比如减小学习率，精心初始化参数，有时还需要 &lt;code&gt;Dropout&lt;/code&gt;  技巧.&lt;/p&gt;
&lt;h2 id=&#34;batch-normalization&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#batch-normalization&#34;&gt;#&lt;/a&gt; Batch Normalization&lt;sup class=&#34;footnote-ref&#34;&gt;&lt;a href=&#34;#fn1&#34; id=&#34;fnref1&#34;&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;
&lt;p&gt;批归一化 &lt;code&gt;BatchNorm&lt;/code&gt; , 一种加速深度神经网络训练速度的方法。同时可以减少 &lt;code&gt;ICS&lt;/code&gt;  问题。可以使得在训练时大胆的使用跟高的学习率和初始化参数，在一定程度上可以具有正则化效果，防止过拟合的功效.（一定程度与 &lt;code&gt;Dropout&lt;/code&gt;  等效） &lt;img data-src=&#34;https://raw.githubusercontent.com/Special-JiaHao/images/main/bn.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;layer-normalization&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#layer-normalization&#34;&gt;#&lt;/a&gt; Layer Normalization&lt;/h2&gt;
&lt;h2 id=&#34;参考文章&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#参考文章&#34;&gt;#&lt;/a&gt; 参考文章&lt;/h2&gt;
&lt;hr class=&#34;footnotes-sep&#34; /&gt;
&lt;section class=&#34;footnotes&#34;&gt;
&lt;ol class=&#34;footnotes-list&#34;&gt;
&lt;li id=&#34;fn1&#34; class=&#34;footnote-item&#34;&gt;&lt;p&gt;&lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzE1MDIuMDMxNjc=&#34;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/span&gt; &lt;a href=&#34;#fnref1&#34; class=&#34;footnote-backref&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
 ]]></description>
        </item>
    </channel>
</rss>
